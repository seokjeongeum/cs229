\begin{answer}
\begin{enumerate}
  \item Using a different constant learning rate does not change the behavior mentioned above.
  \item Decreasing the learning rate over time makes theta almost the same to the previous one which means convergence.
  \item Linear scaling of the input features does not change the behavior mentioned above.
  \item Adding a regularization term $\|\theta\|_2^2$ to the loss function makes $\theta$ to diverge.
  \item Adding zero-mean Gaussian noise to the training data or labels makes the algorithm cannot predict some cases with 100\% confidence.
\end{enumerate}
\end{answer}
